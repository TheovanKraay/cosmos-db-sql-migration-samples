{"cells":[{"cell_type":"code","source":["import uuid\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,DateType,LongType,IntegerType,TimestampType\n\n#JDBC connect details for SQL Server database\njdbcHostname = \"jdbcHostname\"\njdbcDatabase = \"Movies\"\njdbcUsername = \"jdbcUsername\"\njdbcPassword = \"jdbcPassword\"\njdbcPort = \"1433\"\n\nconnectionProperties = {\n  \"user\" : jdbcUsername,\n  \"password\" : jdbcPassword,\n  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n\n#JDBC connect details for Target Azure Cosmos DB SQL API account\njdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, jdbcUsername, jdbcPassword)\nwriteConfig = {\n    \"Endpoint\": \"Endpoint\",\n    \"Masterkey\": \"Masterkey\",\n    \"Database\": \"Movies\",\n    \"Collection\": \"Orders\",\n    \"Upsert\": \"true\"\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["import json\nimport ast\nimport pyspark.sql.functions as F\nimport uuid\nimport numpy as np\nimport pandas as pd\nfrom functools import reduce\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import exp\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import array\nfrom pyspark.sql.types import *\nfrom multiprocessing.pool import ThreadPool\n\n#get all orders\norders = sqlContext.read.jdbc(url=jdbcUrl, table=\"orders\", properties=connectionProperties)\n\n#get all order details\norderdetails = sqlContext.read.jdbc(url=jdbcUrl, table=\"orderdetails\", properties=connectionProperties)\n\n#get all OrderId values to pass to map function \ncols = orders.select('OrderId').collect()\n\n#create thread pool big enough to process merge of details to orders in parallel\npool = ThreadPool(10)\n\ndef writeOrder(col):\n  #filter the order on current value passed from map function\n  order = orders.filter(orders['OrderId'] == col[0])\n  \n  #set id to be a uuid\n  order = order.withColumn(\"id\", lit(str(uuid.uuid1())))\n  \n  #add details field to order dataframe\n  order = order.withColumn(\"details\", lit(''))\n  \n  #filter order details dataframe to get details we want to merge into the order document\n  orderdetailsgroup = orderdetails.filter(orderdetails['OrderId'] == col[0])\n  \n  #convert dataframe to pandas\n  orderpandas = order.toPandas()\n  \n  #convert the order dataframe to json and remove enclosing brackets\n  orderjson = orderpandas.to_json(orient='records', force_ascii=False)\n  orderjson = orderjson[1:-1] \n  \n  #convert orderjson to a dictionaory so we can set the details element with order details later\n  orderjsondata = json.loads(orderjson)\n  \n  \n  #convert orderdetailsgroup dataframe to json, but only if details were returned from the earlier filter\n  if (orderdetailsgroup.count() !=0):\n    #convert orderdetailsgroup to pandas dataframe to work better with json\n    orderdetailsgroup = orderdetailsgroup.toPandas()\n    \n    #convert orderdetailsgroup to json string\n    jsonstring = orderdetailsgroup.to_json(orient='records', force_ascii=False)\n    \n    #convert jsonstring to dictionary to ensure correct encoding and no corrupt records\n    jsonstring = json.loads(jsonstring)\n    \n    #set details json element in orderjsondata to jsonstring which contains orderdetailsgroup - this merges order details into the order \n    orderjsondata['details'] = jsonstring\n \n  #convert dictionary to json\n  orderjsondata = json.dumps(orderjsondata)\n\n  #read the json into spark dataframe\n  df = spark.read.json(sc.parallelize([orderjsondata]))\n  \n  #write the dataframe (this will be a single order record with merged many-to-one order details) to cosmos db using spark the connector\n  #https://docs.microsoft.com/en-us/azure/cosmos-db/spark-connector\n  df.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode(\"append\").options(**writeConfig).save()\n\n#map order details to orders in parallel using the above function\npool.map(writeOrder, cols)\n\n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [None, None, None, None, None, None, None, None, None]</div>"]}}],"execution_count":2}],"metadata":{"name":"sql-to-cosmos-merge-records","notebookId":396657779024245},"nbformat":4,"nbformat_minor":0}
