{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as F\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,DateType,LongType,IntegerType,TimestampType\n\n#JDBC connect details for SQL Server database\njdbcHostname = \"jdbcHostname\"\njdbcDatabase = \"Movies\"\njdbcUsername = \"jdbcUsername\"\njdbcPassword = \"jdbcPassword\"\njdbcPort = \"1433\"\n\nconnectionProperties = {\n  \"user\" : jdbcUsername,\n  \"password\" : jdbcPassword,\n  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n\njdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, jdbcUsername, jdbcPassword)\nwriteConfig = {\n    \"Endpoint\": \"Endpoint\",\n    \"Masterkey\": \"Masterkey\",\n    \"Database\": \"Movies\",\n    \"Collection\": \"Orders\",\n    \"Upsert\": \"true\"\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["import json\nimport ast\nimport pyspark.sql.functions as F\nimport uuid\nimport numpy as np\nfrom functools import reduce\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import exp\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import array\nfrom pyspark.sql.types import *\nfrom multiprocessing.pool import ThreadPool\n\n#get all orders\norders = sqlContext.read.jdbc(url=jdbcUrl, table=\"orders\", properties=connectionProperties)\n\n#get all order details\norderdetails = sqlContext.read.jdbc(url=jdbcUrl, table=\"orderdetails\", properties=connectionProperties)\n\n#get all OrderId values to pass to map function \ncols = orders.select('OrderId').collect()\n\n#create thread pool big enough to process merge of details to orders in parallel\npool = ThreadPool(10)\n\ndef writeOrder(col):\n  #filter the order on current value passed from map function\n  order = orders.filter(orders['OrderId'] == col[0])\n  \n  #set id to be a uuid\n  order = order.withColumn(\"id\", lit(str(uuid.uuid1())))\n  \n  #add details field to order dataframe\n  order = order.withColumn(\"details\", lit(''))\n  \n  #filter order details dataframe to get details we want to merge into the order document\n  orderdetailsgroup = orderdetails.filter(orderdetails['OrderId'] == col[0])\n  \n  #convert dataframe to pandas because of trouble putting array type into spark dataframe field using pyspark\n  orderpandas = order.toPandas()\n  \n  #convert details dataframe to json, but only if details were returned\n  if (orderdetailsgroup.count() !=0):\n    jsonstring = orderdetailsgroup.toJSON().collect()\n    \n    #set details field to be the details json array\n    orderpandas['details'][0] = jsonstring    \n  \n  #convert the order dataframe to json and do some string manipulation to get valid json\n  orderjson = orderpandas.to_json(orient='records')\n  orderjson = reduce(lambda s,r: s.replace(*r),[(\"\\\\\", \"\"),(\"[\\\"\", \"[\"),(\"\\\"]\", \"]\"),(\"}\\\",\\\"{\", \"},{\"),(\"\\\"\", \"'\")], orderjson)\n  orderjson = orderjson[1:-1] \n  \n  #read the json into spark dataframe\n  df = spark.read.json(sc.parallelize([orderjson]))\n  \n  #write the dataframe (this will be a single order record with merged many-to-one order details) to cosmos db using spark connector\n  df.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode(\"append\").options(**writeConfig).save()\n\n#map order details to orders in parallel using the above function\npool.map(writeOrder, cols)\n\n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/tmp/1575663290532-0/PythonShell.py:50: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  from pyspark.context import SparkContext\nOut[4]: [None, None, None, None, None, None, None, None, None]</div>"]}}],"execution_count":2}],"metadata":{"name":"sql-to-cosmos-merge","notebookId":1785025325291023},"nbformat":4,"nbformat_minor":0}
